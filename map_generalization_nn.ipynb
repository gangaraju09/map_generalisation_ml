{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gangaraju09/map_generalisation_ml/blob/main/map_generalization_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWvx3_NqcvPR",
    "outputId": "c4e4603a-c6f1-4c91-d329-db4f10c7404c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd5401ace10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "torch.manual_seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KgSvJLUZc_BH"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM0Bp_A8ysfN",
    "outputId": "067607a1-36d0-401e-e3f8-ed38d91c6a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with their counts are: ['no' 'yes'],[35135   107]\n",
      "Class weights needed for resampling: [1.0030453963284474, 329.3644859813084]\n"
     ]
    }
   ],
   "source": [
    "idaho_df = pd.read_csv('/content/drive/MyDrive/Vertices_Labels/Idaho_vertices_labels.csv')\n",
    "labels_unique, count = np.unique(idaho_df['case'], return_counts=True)\n",
    "\n",
    "print(f\"Number of samples with their counts are: {labels_unique},{count}\")\n",
    "\n",
    "class_weights = [sum(count)/c for c in count]\n",
    "print(f\"Class weights needed for resampling: {class_weights}\")\n",
    "# mapping = {'no': 0, 'yes': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "68LJDxrEdbyL"
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class to load the CSV file\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "      # We read csv file to data.\n",
    "      # 'No' is mapped to 0 and 'Yes' is mapped to 1 to send it as input to Binary cross entropy loss\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.mapping = {'no': 0, 'yes': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "      # Get the lat, long and output of a point\n",
    "        sample = {'lat': self.data.iloc[idx]['Latitude'],\n",
    "                  'long': self.data.iloc[idx]['Longitude'],\n",
    "                  'output': self.mapping[self.data.iloc[idx]['case']]}\n",
    "        return sample\n",
    "\n",
    "# Define the neural network architecture\n",
    "# Declared a simple NN with 2 -> 5 -> 10 -> 5 -> 1 with Sigmoid activation function\n",
    "# TODOs: \n",
    "# 1. Experiment with ReLU\n",
    "# 2. Try Batch Normalization \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 5)\n",
    "        self.bn1 = nn.BatchNorm1d(5)\n",
    "        self.fc2 = nn.Linear(5, 10)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.fc4 = nn.Linear(20, 50)\n",
    "        self.bn4 = nn.BatchNorm1d(50)\n",
    "        self.fc5 = nn.Linear(50, 20)\n",
    "        self.bn5 = nn.BatchNorm1d(20)\n",
    "        self.fc6 = nn.Linear(20, 10)\n",
    "        self.bn6 = nn.BatchNorm1d(10)\n",
    "        self.fc7 = nn.Linear(10, 5)\n",
    "        self.bn7 = nn.BatchNorm1d(5)\n",
    "        self.fc8 = nn.Linear(5, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc8(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jolrBtHj4Pp4",
    "outputId": "903095ef-5ce7-4944-d923-fcb9c7036c2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset is 28193\n",
      "First 10 elements in sampler are: [15128, 3896, 18057, 11374, 13890, 23601, 22059, 23890, 19712, 21648]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and split into training and testing sets\n",
    "dataset = CustomDataset('/content/drive/MyDrive/Vertices_Labels/Idaho_vertices_labels.csv')\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - (train_size)\n",
    "\n",
    "# Creating train and test sets - \n",
    "# TODO: 1. But make sure that train has the minority classes we need!!\n",
    "# 2. Can also create validation dataset!!\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "example_weights = [class_weights[e['output']] for e in train_dataset]\n",
    "\n",
    "print(f\"Length of training dataset is {train_size}\")\n",
    "sampler = WeightedRandomSampler(example_weights, train_size, replacement=True)\n",
    "\n",
    "print(f\"First 10 elements in sampler are: {list(sampler)[:10]}\")\n",
    "# Set up the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, sampler=sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "# num_samples = 32\n",
    "# mini_train_subset = Subset(train_loader.dataset, range(num_samples))\n",
    "# train_loader = DataLoader(mini_train_subset, batch_size=num_samples, sampler=sampler)\n",
    "\n",
    "# mini_test_subset = Subset(test_loader.dataset, range(num_samples))\n",
    "# test_loader = DataLoader(mini_test_subset, batch_size=num_samples)\n",
    "\n",
    "# for data in train_loader:\n",
    "#   print(np.unique(data['output'], return_counts=True))\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xoGNwN8M4UhK"
   },
   "outputs": [],
   "source": [
    "# Set up the neural network and the optimizer\n",
    "net = Net()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsGyUtdu48Nc",
    "outputId": "84ef5dd4-8cf8-4129-95e3-82f20759eefc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-81a7ecaefcaa>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(data['output']).unsqueeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 0.6911\n",
      "Epoch [2], Loss: 0.6898\n",
      "Epoch [3], Loss: 0.6902\n",
      "Epoch [4], Loss: 0.6882\n",
      "Epoch [5], Loss: 0.6875\n",
      "Epoch [6], Loss: 0.6870\n",
      "Epoch [7], Loss: 0.6868\n",
      "Epoch [8], Loss: 0.6872\n",
      "Epoch [9], Loss: 0.6832\n",
      "Epoch [10], Loss: 0.6850\n"
     ]
    }
   ],
   "source": [
    "# Train the neural network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        data['lat'] = data['lat'].float()\n",
    "        data['long'] = data['long'].float()\n",
    "        data['output'] = data['output'].float()\n",
    "        inputs = torch.stack([data['lat'], data['long']], dim=1)\n",
    "        labels = torch.tensor(data['output']).unsqueeze(1)\n",
    "        # print(inputs)\n",
    "        # print(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        # print(outputs)\n",
    "        # break\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('Epoch [%d], Loss: %.4f' % (epoch+1, running_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffp-BVIp4_mU",
    "outputId": "f9be25a4-f4c6-44e9-dfe8-8251c5ef6170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-03832bbe9531>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(data['output']).unsqueeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test points are: 7049 and correct points are 2197\n",
      "Accuracy of the network on the test data: 31 %\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the neural network on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data['lat'] = data['lat'].float()\n",
    "        data['long'] = data['long'].float()\n",
    "        data['output'] = data['output'].float()\n",
    "        inputs = torch.stack([data['lat'], data['long']], dim=1)\n",
    "        labels = torch.tensor(data['output']).unsqueeze(1)\n",
    "        outputs = net(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "torch.save(net, 'best-model.pt')\n",
    "\n",
    "print(f\"Total number of test points are: {total} and correct points are {correct}\")\n",
    "print('Accuracy of the network on the test data: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "g3Ha1MnjdhNT"
   },
   "outputs": [],
   "source": [
    "idaho_train_model = torch.load('best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHHeI-1NlFU2",
    "outputId": "525e207a-ae5b-47ed-b2f5-955e535b1605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3221\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset('/content/drive/MyDrive/Vertices_Labels/Maine_vertices_labels.csv')\n",
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "print(len(dataloader))\n",
    "\n",
    "with open('maine_evaluate.csv', 'a+', newline='') as csvfile:\n",
    "  writer = csv.writer(csvfile, delimiter=',')\n",
    "  writer.writerow(['lat', 'long', 'case'])\n",
    "\n",
    "  with torch.no_grad():\n",
    "      idaho_train_model.eval()\n",
    "      for data in dataloader:\n",
    "          data['lat'] = data['lat'].float()\n",
    "          data['long'] = data['long'].float()\n",
    "          inputs = torch.stack([data['lat'], data['long']], dim=1)\n",
    "          outputs = net(inputs)\n",
    "          predicted = (outputs > 0.5).float()\n",
    "          # predicted = predicted.detach().cpu().numpy()[0]\n",
    "          for i in range(len(data['lat'])):\n",
    "            # print(data['lat'][i].detach().cpu().numpy())\n",
    "            # print(data['long'][i])\n",
    "            # print(predicted[i])\n",
    "            writer.writerow([data['lat'][i].detach().cpu().numpy(), data['long'][i].detach().cpu().numpy(), \n",
    "                             int(predicted[i].detach().cpu().numpy()[0])])\n",
    "  csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hJSaG4FDQxPN"
   },
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MoransI_GWR_Sid.ipynb'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('https://github.com/Sidrcs/Geospatial_BigData_Analytics/raw/main/GeographicWeightedRegression/MoransI_GWR_Sid.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMjxAlA9FystyDZbrH8dXpB",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "19QxlqUE3qOeL_qoJIca3YGYV6i56LV3a",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
